# -*- coding: utf-8 -*-
"""Evaluation Metrics and Regression Implementation Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nJUmMvDPzB_BXSxP7LKgmgjhJXBOvRjJ

**I have built the model in the first part, ignore that.**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sklearn.datasets import fetch_openml

diamond_data = fetch_openml(name='diamonds', version=1)

diamond_data.DESCR

diamond_data

diamond=pd.DataFrame(diamond_data.data,columns=diamond_data.feature_names)

diamond['price']=diamond_data.target

diamond

diamond.info()

from sklearn.preprocessing import OrdinalEncoder

encoder= OrdinalEncoder()

encoder

diamond['cut'].unique()



encoder = OrdinalEncoder(categories=[["Fair", "Good", "Very Good", "Premium", "Ideal"]])

cut=encoder.fit_transform(diamond[["cut"]])
cut

diamond['cut_encoded']=cut

diamond

diamond['clarity'].unique()

encoder = OrdinalEncoder(categories=[['I1', 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF']])
clarity=encoder.fit_transform(diamond[["clarity"]])
print(clarity)
diamond['clarity_encoded']=clarity

diamond

diamond['color'].unique()

encoder = OrdinalEncoder(categories=[['J', 'I', 'H', 'G', 'F', 'E', 'D']])
color=encoder.fit_transform(diamond[["color"]])
print(color)
diamond['color_encoded']=color

diamond

df=diamond.drop(['cut','clarity','color'],axis=1)

df

df.info()

df.describe()

df.corr()

df.drop('price',axis=1,inplace=True)

df['price']=diamond_data.target

df.head()

df.dropna()

df.drop_duplicates(inplace=True)

sns.heatmap(df.corr(),annot=True)

df.shape

x=df.iloc[:,:-1]
y=df.iloc[:,-1]

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)

from sklearn.preprocessing import StandardScaler

scaler=StandardScaler()

x_train=pd.DataFrame(scaler.fit_transform(x_train),columns=x_train.columns)

x_test=scaler.transform(x_test)

x_train

x_test

from sklearn.linear_model import LinearRegression
model = LinearRegression()

model.fit(x_train, y_train)

model.coef_

model.intercept_

y_pred=model.predict(x_test)
y_pred

from sklearn.metrics import r2_score

r2=r2_score(y_test,y_pred)

r2

# 90% yehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh!!

"""**Assignment Starts Here**

**1. Write a Python script to visualize the distribution of errors (residuals) for a multiple linear regression model
using Seaborn's "diamonds" dataset.**
"""

residuals = y_test - y_pred

# Create a Seaborn plot to visualize the distribution of residuals
sns.set(style="whitegrid")
plt.figure(figsize=(10, 6))

# Plot the residuals distribution
sns.histplot(residuals, kde=True, color="blue", bins=30)

# Add titles and labels
plt.title("Distribution of Residuals", fontsize=16)
plt.xlabel("Residuals", fontsize=14)
plt.ylabel("Frequency", fontsize=14)

# Show the plot
plt.show()

"""**2. Write a Python script to calculate and print Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root
Mean Squared Error (RMSE) for a linear regression model.**
"""

from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np

# Assuming you have your true values in `y_true` and predicted values in `y_pred`

# Calculate MSE, MAE, and RMSE
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mse)

# Print the results
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")

"""**3. Write a Python script to check if the assumptions of linear regression are met. Use a scatter plot to check
linearity, residuals plot for homoscedasticity, and correlation matrix for multicollinearity.**
"""

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# Assuming `df` is your dataset and 'target' is the target variable

# 1. Check linearity - Scatter plots for each feature vs target
plt.figure(figsize=(12, 8))

# Scatter plot for each feature vs target
for i, column in enumerate(df.columns[:-1]):  # Assuming target is the last column
    sns.scatterplot(x=df[column], y=df['price'])
    plt.title(f'{column} vs Target')

plt.tight_layout()
plt.show()

# 2. Check homoscedasticity - Residuals plot
# Assuming you have your model's predictions and true values
y_test = df['price']
X = df.drop('price', axis=1)
model = LinearRegression().fit(X, y_test)
y_pred = model.predict(X)
residuals = y_test - y_pred

plt.figure(figsize=(8, 6))
sns.residplot(x=y_pred, y=residuals, lowess=True, color='blue', line_kws={'color': 'red'})
plt.title("Residuals Plot (Homoscedasticity Check)")
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.show()

# 3. Check multicollinearity - Correlation Matrix
plt.figure(figsize=(10, 8))
correlation_matrix = df.drop('price', axis=1).corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title("Correlation Matrix (Multicollinearity Check)")
plt.show()

"""**4. Write a Python script that creates a machine learning pipeline with feature scaling and evaluates the
performance of different regression models.**
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.pipeline import Pipeline

# Assuming 'df' is your dataset and 'price' is the target column

# Step 1: Split the data into training and testing sets
X = df.drop('price', axis=1)  # Features
y = df['price']  # Target variable (price)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 2: Create a pipeline with feature scaling and regression models

# Initialize a scaler
scaler = StandardScaler()

# List of regression models to evaluate
models = [
    ('Linear Regression', LinearRegression()),
    ('Ridge Regression', Ridge()),
    ('Lasso Regression', Lasso()),
    ('Support Vector Regression', SVR()),
    ('Decision Tree Regressor', DecisionTreeRegressor()),
    ('Random Forest Regressor', RandomForestRegressor())
]

# Step 3: Evaluate the models
for model_name, model in models:
    # Create a pipeline with scaling and model
    pipeline = Pipeline([
        ('scaler', scaler),
        ('regressor', model)
    ])

    # Train the model
    pipeline.fit(X_train, y_train)

    # Make predictions
    y_pred = pipeline.predict(X_test)

    # Check the shapes of y_test and y_pred to make sure they're the same
    print(f"Shape of y_test: {y_test.shape}")
    print(f"Shape of y_pred: {y_pred.shape}")

    # Ensure y_pred is 1D, in case it's not
    if y_pred.ndim > 1:
        y_pred = y_pred.flatten()

    # Evaluate performance using different metrics
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Print evaluation metrics
    print(f'{model_name}:')
    print(f'Mean Squared Error (MSE): {mse:.4f}')
    print(f'Mean Absolute Error (MAE): {mae:.4f}')
    print(f'R-squared (R2): {r2:.4f}')
    print('-' * 50)

"""**5. Implement a simple linear regression model on a dataset and print the model's coefficients, intercept, and
R-squared score.**
"""

#already done in the model I build

'''
model.coef_
array([ 5116.67394805,  -120.28825524,   -59.21889061, -1005.79844103,
          28.69026596,    15.67219402,   134.37755676,   827.46242009,
         547.44957595])

model.intercept_
3944.8832345765077

r2
0.9079726194609131
'''

"""**6. Write a Python script that analyzes the relationship between total bill and tip in the 'tips' dataset using
simple linear regression and visualizes the results.**
"""

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Load the 'tips' dataset from Seaborn
tips = sns.load_dataset('tips')

# Step 1: Prepare the data (total_bill as feature and tip as target)
X = tips[['total_bill']]  # Feature (total_bill)
y = tips['tip']  # Target (tip)

# Step 2: Create and train the simple linear regression model
model = LinearRegression()
model.fit(X, y)

# Step 3: Predict the target values
y_pred = model.predict(X)

# Step 4: Calculate the R-squared score
r2 = r2_score(y, y_pred)

# Step 5: Visualize the data and the linear regression result
plt.figure(figsize=(10, 6))
sns.scatterplot(x='total_bill', y='tip', data=tips, color='blue', label='Data points')
plt.plot(X, y_pred, color='red', label='Regression Line')
plt.title(f'Relationship between Total Bill and Tip (R-squared = {r2:.4f})')
plt.xlabel('Total Bill')
plt.ylabel('Tip')
plt.legend()
plt.show()

# Print the model's coefficients, intercept, and R-squared score
print(f'Coefficients: {model.coef_}')
print(f'Intercept: {model.intercept_}')
print(f'R-squared: {r2:.4f}')

"""**7. Write a Python script that fits a linear regression model to a synthetic dataset with one feature. Use the
model to predict new values and plot the data points along with the regression line.**
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Step 1: Create a synthetic dataset with one feature (X) and target (y)
np.random.seed(42)  # For reproducibility
X = 2 * np.random.rand(100, 1)  # Random values for the feature (100 data points)
y = 4 + 3 * X + np.random.randn(100, 1)  # Linear relationship with some noise

# Step 2: Create and train a linear regression model
model = LinearRegression()
model.fit(X, y)

# Step 3: Make predictions using the trained model
y_pred = model.predict(X)

# Step 4: Plot the data points and the regression line
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='blue', label='Data points')
plt.plot(X, y_pred, color='red', label='Regression Line')
plt.title('Linear Regression on Synthetic Dataset')
plt.xlabel('Feature (X)')
plt.ylabel('Target (y)')
plt.legend()
plt.show()

# Step 5: Print the model's coefficients and intercept
print(f'Coefficient: {model.coef_[0][0]}')
print(f'Intercept: {model.intercept_[0]}')

"""**8. Write a Python script that pickles a trained linear regression model and saves it to a file**"""

import pickle
import numpy as np
from sklearn.linear_model import LinearRegression

# Step 1: Create a synthetic dataset
np.random.seed(42)  # For reproducibility
X = 2 * np.random.rand(100, 1)  # Random values for the feature (100 data points)
y = 4 + 3 * X + np.random.randn(100, 1)  # Linear relationship with some noise

# Step 2: Create and train a linear regression model
model = LinearRegression()
model.fit(X, y)

# Step 3: Pickle the trained model and save it to a file
filename = 'linear_regression_model.pkl'

# Open the file in write-binary mode and pickle the model
with open(filename, 'wb') as file:
    pickle.dump(model, file)

print(f'Model successfully pickled and saved to {filename}')

"""**9. Write a Python script that fits a polynomial regression model (degree 2) to a dataset and plots the
regression curve.**
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

# Step 1: Create a synthetic dataset (you can modify it as needed)
np.random.seed(42)  # For reproducibility
X = np.linspace(-3, 3, 100).reshape(-1, 1)  # Generate 100 points between -3 and 3
y = X**2 + np.random.randn(100, 1)  # Quadratic relationship with some noise

# Step 2: Transform the feature into a higher degree (degree 2) for polynomial regression
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)  # Transform the original X into a polynomial feature matrix

# Step 3: Fit the Polynomial Regression model
model = LinearRegression()
model.fit(X_poly, y)

# Step 4: Make predictions
y_pred = model.predict(X_poly)

# Step 5: Plot the original data points and the polynomial regression curve
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='blue', label='Data points')
plt.plot(X, y_pred, color='red', label='Polynomial Regression Curve (Degree 2)')
plt.title('Polynomial Regression (Degree 2)')
plt.xlabel('Feature (X)')
plt.ylabel('Target (y)')
plt.legend()
plt.show()

# Step 6: Print the model's coefficients and intercept
print(f'Coefficient: {model.coef_}')
print(f'Intercept: {model.intercept_}')

"""**10. Generate synthetic data for simple linear regression (use random values for X and y) and fit a linear
regression model to the data. Print the model's coefficient and intercept.**
"""

import numpy as np
from sklearn.linear_model import LinearRegression

# Step 1: Generate synthetic data for simple linear regression
np.random.seed(42)  # For reproducibility

# Generate 100 random data points for X between 0 and 10
X = np.random.rand(100, 1) * 10

# Generate corresponding y values using a linear relationship with some noise
# y = 3 * X + noise (noise added using normal distribution)
y = 3 * X + np.random.randn(100, 1) * 2  # Adding some noise to make it more realistic

# Step 2: Create a linear regression model and fit it to the data
model = LinearRegression()
model.fit(X, y)

# Step 3: Print the model's coefficient and intercept
print(f'Coefficient: {model.coef_[0]}')
print(f'Intercept: {model.intercept_}')

"""**11. Write a Python script that fits polynomial regression models of different degrees to a synthetic dataset and
compares their performance.**
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error

# Step 1: Generate synthetic data for polynomial regression
np.random.seed(42)  # For reproducibility
X = np.linspace(-3, 3, 100).reshape(-1, 1)  # Generate 100 points between -3 and 3
y = X**3 + np.random.randn(100, 1) * 5  # Cubic relationship with some noise

# Step 2: Function to fit polynomial regression and calculate MSE
def fit_polynomial_regression(X, y, degree):
    # Step 2a: Transform the feature into polynomial features of the given degree
    poly = PolynomialFeatures(degree=degree)
    X_poly = poly.fit_transform(X)

    # Step 2b: Fit the polynomial regression model
    model = LinearRegression()
    model.fit(X_poly, y)

    # Step 2c: Make predictions
    y_pred = model.predict(X_poly)

    # Step 2d: Calculate MSE
    mse = mean_squared_error(y, y_pred)

    return model, mse, y_pred

# Step 3: Fit models for degrees 1, 2, and 3 and calculate their MSE
degrees = [1, 2, 3]
models = []
mses = []
predictions = []
for degree in degrees:
    model, mse, y_pred = fit_polynomial_regression(X, y, degree)
    models.append(model)
    mses.append(mse)
    predictions.append(y_pred)

# Step 4: Plot the data and regression curves for each degree
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='blue', label='Data points')

# Plot the polynomial regression curves for different degrees
for i, degree in enumerate(degrees):
    plt.plot(X, predictions[i], label=f'Degree {degree} (MSE: {mses[i]:.2f})')

plt.title('Polynomial Regression Models of Different Degrees')
plt.xlabel('Feature (X)')
plt.ylabel('Target (y)')
plt.legend()
plt.show()

# Step 5: Print the MSE for each model
for degree, mse in zip(degrees, mses):
    print(f'Degree {degree}: Mean Squared Error = {mse:.2f}')

"""**12. Write a Python script that fits a simple linear regression model with two features and prints the model's
coefficients, intercept, and R-squared score.**
"""

import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Step 1: Generate synthetic data with two features
np.random.seed(42)  # For reproducibility

# Generate random data for two features (X1 and X2)
X = np.random.rand(100, 2) * 10  # 100 samples with 2 features each

# Generate a target variable y with a linear relationship to X1 and X2
# y = 3 * X1 + 5 * X2 + some noise
y = 3 * X[:, 0] + 5 * X[:, 1] + np.random.randn(100) * 2  # Adding some noise

# Step 2: Create and fit the linear regression model
model = LinearRegression()
model.fit(X, y)

# Step 3: Print the model's coefficients, intercept, and R-squared score
print(f'Coefficients: {model.coef_}')
print(f'Intercept: {model.intercept_}')
print(f'R-squared score: {model.score(X, y)}')

"""**13. Write a Python script that generates synthetic data, fits a linear regression model, and visualizes the
regression line along with the data points.**
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Step 1: Generate synthetic data
np.random.seed(42)  # For reproducibility

# Generate random data for X (feature)
X = np.random.rand(100, 1) * 10  # 100 data points between 0 and 10

# Generate a target variable y with a linear relationship to X
# y = 3 * X + some noise
y = 3 * X + np.random.randn(100, 1) * 2  # Adding some noise to make it more realistic

# Step 2: Fit the linear regression model
model = LinearRegression()
model.fit(X, y)

# Step 3: Make predictions
y_pred = model.predict(X)

# Step 4: Plot the data points and regression line
plt.figure(figsize=(8, 6))

# Scatter plot of the data points
plt.scatter(X, y, color='blue', label='Data points')

# Plot the regression line
plt.plot(X, y_pred, color='red', label='Regression line')

# Labels and title
plt.xlabel('Feature (X)')
plt.ylabel('Target (y)')
plt.title('Linear Regression: Data Points and Regression Line')
plt.legend()

# Show the plot
plt.show()

# Step 5: Print the model's coefficients and intercept
print(f'Coefficient: {model.coef_[0]}')
print(f'Intercept: {model.intercept_}')

"""**14. Write a Python script that uses the Variance Inflation Factor (VIF) to check for multicollinearity in a dataset
with multiple features.**
"""

import numpy as np
import pandas as pd
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

# Step 1: Load the dataset
# For example, let's use the 'diamonds' dataset from seaborn
import seaborn as sns
diamonds = sns.load_dataset('diamonds')

# Step 2: Preprocess the dataset (select numeric features)
# We'll select the numeric columns for this demonstration
numeric_data = diamonds.select_dtypes(include=[np.number]).dropna()

# Step 3: Add constant (intercept) for the VIF calculation
X = add_constant(numeric_data)  # Adds a column of 1's for the intercept term

# Step 4: Calculate the VIF for each feature
vif_data = pd.DataFrame()
vif_data["Feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# Step 5: Display the VIF for each feature
print(vif_data)

"""**15. Write a Python script that generates synthetic data for a polynomial relationship (degree 4), fits a
polynomial regression model, and plots the regression curve.**
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

# Step 1: Generate synthetic data for a polynomial relationship (degree 4)
np.random.seed(42)  # For reproducibility

# Generate random data for X
X = np.random.rand(100, 1) * 10  # 100 data points between 0 and 10

# Generate a target variable y with a polynomial relationship (degree 4)
y = 2 * X**4 - 5 * X**3 + 3 * X**2 + 4 * X + np.random.randn(100, 1) * 100  # Adding noise

# Step 2: Transform the features to include polynomial terms (degree 4)
poly = PolynomialFeatures(degree=4)
X_poly = poly.fit_transform(X)

# Step 3: Fit the polynomial regression model
model = LinearRegression()
model.fit(X_poly, y)

# Step 4: Predict the values using the trained model
y_pred = model.predict(X_poly)

# Step 5: Plot the data points and the regression curve
plt.figure(figsize=(8, 6))

# Scatter plot of the original data points
plt.scatter(X, y, color='blue', label='Data points')

# Plot the regression curve
X_range = np.linspace(0, 10, 1000).reshape(-1, 1)
X_range_poly = poly.transform(X_range)
y_range_pred = model.predict(X_range_poly)
plt.plot(X_range, y_range_pred, color='red', label='Polynomial Regression Curve')

# Labels and title
plt.xlabel('Feature (X)')
plt.ylabel('Target (y)')
plt.title('Polynomial Regression (Degree 4): Data Points and Regression Curve')
plt.legend()

# Show the plot
plt.show()

# Step 6: Print the model's coefficients and intercept
print(f'Coefficients: {model.coef_}')
print(f'Intercept: {model.intercept_}')

"""**16. Write a Python script that creates a machine learning pipeline with data standardization and a multiple
linear regression model, and prints the R-squared score.**
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline
from sklearn.metrics import r2_score

# Step 1: Load the dataset
# We'll use the 'diamonds' dataset from seaborn as an example
import seaborn as sns
diamonds = sns.load_dataset('diamonds')

# Step 2: Preprocess the dataset (select numeric features and drop any rows with missing values)
numeric_data = diamonds.select_dtypes(include=[np.number]).dropna()

# Features and target variable
X = numeric_data.drop('price', axis=1)  # Features (all columns except 'price')
y = numeric_data['price']  # Target variable (price)

# Step 3: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Create a machine learning pipeline with data standardization and a multiple linear regression model
pipeline = make_pipeline(
    StandardScaler(),  # Standardize the features
    LinearRegression()  # Fit a linear regression model
)

# Step 5: Train the model
pipeline.fit(X_train, y_train)

# Step 6: Predict on the test data
y_pred = pipeline.predict(X_test)

# Step 7: Calculate and print the R-squared score
r2 = r2_score(y_test, y_pred)
print(f'R-squared score: {r2:.4f}')

"""**17. Write a Python script that performs polynomial regression (degree 3) on a synthetic dataset and plots the
regression curve.**
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

# Step 1: Generate synthetic data for a polynomial relationship
np.random.seed(42)  # For reproducibility

# Generate random data for X
X = np.random.rand(100, 1) * 10  # 100 data points between 0 and 10

# Generate a target variable y with a polynomial relationship (degree 3)
y = 3 * X**3 - 8 * X**2 + 2 * X + np.random.randn(100, 1) * 30  # Adding noise

# Step 2: Transform the features to include polynomial terms (degree 3)
poly = PolynomialFeatures(degree=3)
X_poly = poly.fit_transform(X)

# Step 3: Fit the polynomial regression model
model = LinearRegression()
model.fit(X_poly, y)

# Step 4: Predict the values using the trained model
y_pred = model.predict(X_poly)

# Step 5: Plot the data points and the regression curve
plt.figure(figsize=(8, 6))

# Scatter plot of the original data points
plt.scatter(X, y, color='blue', label='Data points')

# Plot the regression curve
X_range = np.linspace(0, 10, 1000).reshape(-1, 1)
X_range_poly = poly.transform(X_range)
y_range_pred = model.predict(X_range_poly)
plt.plot(X_range, y_range_pred, color='red', label='Polynomial Regression Curve')

# Labels and title
plt.xlabel('Feature (X)')
plt.ylabel('Target (y)')
plt.title('Polynomial Regression (Degree 3): Data Points and Regression Curve')
plt.legend()

# Show the plot
plt.show()

# Step 6: Print the model's coefficients and intercept
print(f'Coefficients: {model.coef_}')
print(f'Intercept: {model.intercept_}')

"""**18. Write a Python script that performs multiple linear regression on a synthetic dataset with 5 features. Print
the R-squared score and model coefficients**
"""

import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

# Step 1: Generate synthetic data with 5 features
np.random.seed(42)  # For reproducibility

# Generate random data for 5 features (X)
X = np.random.rand(100, 5) * 10  # 100 data points, each with 5 features between 0 and 10

# Generate a target variable y with a linear relationship
# y = 3*X1 + 2*X2 - X3 + 4*X4 - 5*X5 + some noise
y = 3 * X[:, 0] + 2 * X[:, 1] - X[:, 2] + 4 * X[:, 3] - 5 * X[:, 4] + np.random.randn(100) * 5

# Step 2: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Fit the multiple linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Step 4: Predict on the test data
y_pred = model.predict(X_test)

# Step 5: Calculate and print the R-squared score
r2 = r2_score(y_test, y_pred)
print(f'R-squared score: {r2:.4f}')

# Step 6: Print the model's coefficients
print('Model Coefficients:', model.coef_)
print('Model Intercept:', model.intercept_)

"""**19. Write a Python script that generates synthetic data for linear regression, fits a model, and visualizes the
data points along with the regression line.**
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Step 1: Generate synthetic data for linear regression
np.random.seed(42)  # For reproducibility

# Generate random data for X (100 data points between 0 and 10)
X = np.random.rand(100, 1) * 10

# Generate the target variable y with a linear relationship (y = 2 * X + noise)
y = 2 * X + np.random.randn(100, 1) * 3  # Adding noise to simulate real-world data

# Step 2: Fit the linear regression model
model = LinearRegression()
model.fit(X, y)

# Step 3: Predict values using the trained model
y_pred = model.predict(X)

# Step 4: Visualize the data points and the regression line
plt.figure(figsize=(8, 6))

# Scatter plot of the original data points
plt.scatter(X, y, color='blue', label='Data points')

# Plot the regression line
plt.plot(X, y_pred, color='red', label='Regression Line')

# Labels and title
plt.xlabel('Feature (X)')
plt.ylabel('Target (y)')
plt.title('Linear Regression: Data Points and Regression Line')
plt.legend()

# Show the plot
plt.show()

# Step 5: Print the model's coefficients and intercept
print(f'Coefficient: {model.coef_[0]}')
print(f'Intercept: {model.intercept_}')

"""**20. Create a synthetic dataset with 3 features and perform multiple linear regression. Print the model's Rsquared score and coefficients.**"""

import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

# Step 1: Generate synthetic data with 3 features
np.random.seed(42)  # For reproducibility

# Generate random data for 3 features (X) with 100 data points
X = np.random.rand(100, 3) * 10  # 100 data points, each with 3 features between 0 and 10

# Generate a target variable y with a linear relationship
# y = 4*X1 + 2*X2 - X3 + noise
y = 4 * X[:, 0] + 2 * X[:, 1] - X[:, 2] + np.random.randn(100) * 5

# Step 2: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Fit the multiple linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Step 4: Predict on the test data
y_pred = model.predict(X_test)

# Step 5: Calculate and print the R-squared score
r2 = r2_score(y_test, y_pred)
print(f'R-squared score: {r2:.4f}')

# Step 6: Print the model's coefficients and intercept
print('Model Coefficients:', model.coef_)
print('Model Intercept:', model.intercept_)

"""**21. Write a Python script that demonstrates how to serialize and deserialize machine learning models using
joblib instead of pickling.**
"""

import joblib
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Step 1: Generate synthetic data
np.random.seed(42)

# Generate random data for 3 features (X) with 100 data points
X = np.random.rand(100, 3) * 10  # 100 data points, each with 3 features between 0 and 10

# Generate a target variable y with a linear relationship
y = 4 * X[:, 0] + 2 * X[:, 1] - X[:, 2] + np.random.randn(100) * 5

# Step 2: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Fit the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Step 4: Serialize the trained model using joblib
joblib.dump(model, 'linear_regression_model.joblib')
print("Model has been serialized and saved.")

# Step 5: Deserialize the model from the file
loaded_model = joblib.load('linear_regression_model.joblib')
print("Model has been deserialized.")

# Step 6: Test the deserialized model
y_pred = loaded_model.predict(X_test)

# Print the R-squared score to verify if the deserialized model performs correctly
from sklearn.metrics import r2_score
print(f'R-squared score of the deserialized model: {r2_score(y_test, y_pred):.4f}')

"""**22. Write a Python script to perform linear regression with categorical features using one-hot encoding. Use
the Seaborn 'tips' dataset.**
"""

import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, r2_score

# Step 1: Load the Seaborn 'tips' dataset
tips = sns.load_dataset('tips')

# Step 2: Preprocess the data
# We'll predict the 'tip' using the 'total_bill', 'sex', 'smoker', 'day', 'time', and 'size' columns
# Convert categorical features to one-hot encoding
X = tips[['total_bill', 'sex', 'smoker', 'day', 'time', 'size']]
y = tips['tip']

# Step 3: Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Create a column transformer for one-hot encoding of categorical features
# Apply one-hot encoding to categorical features: 'sex', 'smoker', 'day', 'time'
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(), ['sex', 'smoker', 'day', 'time'])
    ],
    remainder='passthrough'  # Leave 'total_bill' and 'size' as they are
)

# Step 5: Create a pipeline with preprocessing and linear regression model
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', LinearRegression())
])

# Step 6: Train the model
pipeline.fit(X_train, y_train)

# Step 7: Make predictions on the test data
y_pred = pipeline.predict(X_test)

# Step 8: Evaluate the model's performance
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse:.4f}')
print(f'R-squared score: {r2:.4f}')

# Step 9: Print model coefficients and intercept
print(f'Intercept: {pipeline.named_steps["regressor"].intercept_}')
print(f'Coefficients: {pipeline.named_steps["regressor"].coef_}')

"""**23. Compare Ridge Regression with Linear Regression on a synthetic dataset and print the coefficients and Rsquared score.**"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

# Step 1: Generate synthetic data for linear regression
np.random.seed(42)
X = np.random.rand(100, 1) * 10  # 100 random values for X between 0 and 10
y = 2 * X + 3 + np.random.randn(100, 1) * 2  # y = 2 * X + 3 with some noise

# Step 2: Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Fit Linear Regression model
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

# Step 4: Fit Ridge Regression model (with alpha = 1.0)
ridge_model = Ridge(alpha=1.0)
ridge_model.fit(X_train, y_train)

# Step 5: Make predictions using both models
y_pred_linear = linear_model.predict(X_test)
y_pred_ridge = ridge_model.predict(X_test)

# Step 6: Evaluate the models' performance using R-squared score
r2_linear = r2_score(y_test, y_pred_linear)
r2_ridge = r2_score(y_test, y_pred_ridge)

# Step 7: Print the model coefficients and R-squared scores
print("Linear Regression:")
print(f"Coefficients: {linear_model.coef_}")
print(f"Intercept: {linear_model.intercept_}")
print(f"R-squared score: {r2_linear:.4f}\n")

print("Ridge Regression:")
print(f"Coefficients: {ridge_model.coef_}")
print(f"Intercept: {ridge_model.intercept_}")
print(f"R-squared score: {r2_ridge:.4f}")

# Step 8: Visualize the comparison between the models
plt.scatter(X_test, y_test, color='blue', label='Actual data', alpha=0.6)
plt.plot(X_test, y_pred_linear, color='green', label='Linear Regression', linewidth=2)
plt.plot(X_test, y_pred_ridge, color='red', label='Ridge Regression', linewidth=2)
plt.xlabel('X')
plt.ylabel('y')
plt.title('Comparison between Linear Regression and Ridge Regression')
plt.legend()
plt.show()

"""**24. Write a Python script that uses cross-validation to evaluate a Linear Regression model on a synthetic
dataset.**
"""

import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.datasets import make_regression

# Step 1: Generate synthetic data for linear regression
X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)

# Step 2: Create a Linear Regression model
linear_model = LinearRegression()

# Step 3: Use cross-validation to evaluate the model
cv_scores = cross_val_score(linear_model, X, y, cv=5, scoring='neg_mean_squared_error')

# Step 4: Print the cross-validation results
print(f"Cross-validation scores (Negative MSE): {cv_scores}")
print(f"Mean of cross-validation scores (Negative MSE): {cv_scores.mean()}")
print(f"Standard deviation of cross-validation scores (Negative MSE): {cv_scores.std()}")

# Step 5: Convert the negative MSE to positive MSE
cv_scores_positive = -cv_scores

# Step 6: Print positive Mean Squared Error (MSE)
print(f"Cross-validation Mean Squared Errors (MSE): {cv_scores_positive}")
print(f"Mean of MSE: {cv_scores_positive.mean()}")

"""**25. Write a Python script that compares polynomial regression models of different degrees and prints the Rsquared score for each.**"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

# Step 1: Generate synthetic data for linear regression
X = np.random.rand(100, 1) * 10  # 100 random values between 0 and 10
y = 3 * X**2 + 5 + np.random.randn(100, 1) * 5  # Quadratic relationship with noise

# Step 2: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Function to evaluate polynomial regression models of different degrees
def evaluate_polynomial_regression(X_train, X_test, y_train, y_test, degrees):
    for degree in degrees:
        # Step 4: Polynomial feature transformation
        poly = PolynomialFeatures(degree=degree)
        X_poly_train = poly.fit_transform(X_train)
        X_poly_test = poly.transform(X_test)

        # Step 5: Train a Linear Regression model on the transformed features
        model = LinearRegression()
        model.fit(X_poly_train, y_train)

        # Step 6: Predict and evaluate the model
        y_pred = model.predict(X_poly_test)
        r2 = r2_score(y_test, y_pred)

        # Step 7: Print the R-squared score for the current degree
        print(f"Degree {degree} - R-squared score: {r2:.4f}")

        # Step 8: Plot the results for visualization
        plt.figure(figsize=(10, 6))
        plt.scatter(X_test, y_test, color='blue', label='True values')
        plt.scatter(X_test, y_pred, color='red', label='Predictions')
        plt.plot(np.sort(X_test, axis=0), model.predict(poly.transform(np.sort(X_test, axis=0))), color='green', label=f'Degree {degree} Regression Line')
        plt.title(f'Polynomial Regression (Degree {degree})')
        plt.xlabel('X')
        plt.ylabel('y')
        plt.legend()
        plt.show()

# Step 9: Specify the degrees of polynomial models to evaluate
degrees = [1, 2, 3, 4]

# Step 10: Evaluate the models
evaluate_polynomial_regression(X_train, X_test, y_train, y_test, degrees)

