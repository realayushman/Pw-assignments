# -*- coding: utf-8 -*-
"""Logistic regression assign.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10Ad6vxwkitR0JQiCS2L6iKEEiOeQ5iQI

Alright! Letâ€™s tackle all the questions one by one, breaking them down clearly with examples where needed. Letâ€™s roll! ðŸš€  

---

### 1. **What is Logistic Regression, and how does it differ from Linear Regression?**  

- **Logistic Regression:** A classification algorithm used to predict the probability of a target variable belonging to a particular class (usually binary â€” 0 or 1).  
- **Linear Regression:** A regression algorithm used to predict continuous values by fitting a straight line through the data.  

**Key difference:**  
- **Linear Regression** outputs continuous values, while **Logistic Regression** outputs probabilities and maps them to class labels using a threshold (usually 0.5).  

**Example:**  
- **Linear Regression:** Predict a studentâ€™s exam score (continuous value).  
- **Logistic Regression:** Predict whether a student passes or fails the exam (binary outcome).  

---

### 2. **What is the mathematical equation of Logistic Regression?**  

The probability that a given input `x` belongs to class 1 is calculated using the **sigmoid function**:

P(y=1|x) = 1/(1 + e^{-(beta_0 + beta_1.X_1 + ... + \beta_n.X_n)})

Where:  
- \( y \): target variable (0 or 1)  
- \( beta_0 \): intercept term  
- \( beta_1, beta_2, ... \): coefficients for input features \( x_1, x_2, ... \)  

---

### 3. **Why do we use the Sigmoid function in Logistic Regression?**  

The sigmoid function:


sigma(z) = 1/(1 + e^{-z})

is used to map any real number to a value between 0 and 1, representing a probability.  

- If \( sigma(z) \) > 0.5 â†’ classify as 1  
- If \( sigma(z) \) â‰¤ 0.5 â†’ classify as 0  

**Why not use linear functions?**  
A linear function can produce outputs outside the range of 0 to 1, making it unsuitable for probabilities. The sigmoid "squashes" all outputs into the valid probability range.

---

### 4. **What is the cost function of Logistic Regression?**  

Unlike linear regressionâ€™s Mean Squared Error (MSE), **Logistic Regression** uses **log loss (cross-entropy loss)**:

J(Î¸) = - (1/m) * Î£ [y_i * log(h(x_i)) + (1 - y_i) * log(1 - h(x_i))]

Why not MSE?  
- MSE results in a non-convex function for logistic regression, making it hard to optimize.  
- **Log loss** is convex, allowing gradient descent to find the global minimum.  

---

### 5. **What is Regularization in Logistic Regression? Why is it needed?**  

**Regularization** adds a penalty to the cost function to prevent **overfitting** by discouraging overly complex models.  

- **L1 regularization (Lasso):** Adds the absolute value of the coefficients:  
J(Î¸) = - log likelihood + Î» * Î£ |Î²_j|

- **L2 regularization (Ridge):** Adds the square of the coefficients:  
J(Î¸) = - log likelihood + Î» * Î£ Î²_j^2


**Why needed?**  
Without regularization, Logistic Regression might fit the noise in the data, leading to poor generalization on unseen data.

---

### 6. **Explain the difference between Lasso, Ridge, and Elastic Net regression.**  

- **Ridge Regression (L2):** Adds **squared magnitude** of coefficients as a penalty. It reduces all coefficients but doesnâ€™t shrink any to zero.  
- **Lasso Regression (L1):** Adds **absolute values** of coefficients as a penalty. It can shrink some coefficients **exactly to zero** â€” useful for feature selection.  
- **Elastic Net:** A combination of Lasso and Ridge, controlled by a parameter \( \alpha \):  
Î» * (Î± * Î£ |Î²_j| + (1 - Î±) * Î£ Î²_j^2)

---

### 7. **When should we use Elastic Net instead of Lasso or Ridge?**  

- Use **Ridge** when most features have small effects (many small coefficients).  
- Use **Lasso** when some features are irrelevant (feature selection).  
- Use **Elastic Net** when there are **many correlated features** â€” it balances Lassoâ€™s sparsity with Ridgeâ€™s stability.

---

### 8. **What is the impact of the regularization parameter (Î») in Logistic Regression?**  

- **Higher \( \lambda \):** Stronger regularization â†’ simpler model (coefficients shrink).  
- **Lower \( \lambda \):** Weaker regularization â†’ more complex model (coefficients grow).  
- **\( \lambda = 0 \):** No regularization â†’ behaves like plain Logistic Regression.

---

### 9. **What are the key assumptions of Logistic Regression?**  

1. **Linearity of log-odds:** The relationship between features and log-odds of the outcome is linear.  
2. **No multicollinearity:** Features should not be highly correlated with each other.  
3. **Independence of observations:** Observations should be independent of each other.  
4. **Sufficient sample size:** Requires enough samples for stable coefficient estimates.  

---

### 10. **What are some alternatives to Logistic Regression for classification tasks?**  

- **Decision Trees**  
- **Random Forest**  
- **Support Vector Machines (SVM)**  
- **K-Nearest Neighbors (KNN)**  
- **Naive Bayes**  
- **Neural Networks**  

---

Gotcha! Letâ€™s clean this up â€” Iâ€™ll keep the answers crisp, clear, and easy to skim through. Letâ€™s go! ðŸš€  

---

### 11. What are Classification Evaluation Metrics?

- **Accuracy** = (TP + TN) / (TP + TN + FP + FN)  
- **Precision** = TP / (TP + FP) â€” Focuses on how many predicted positives are correct.  
- **Recall (Sensitivity)** = TP / (TP + FN) â€” Focuses on capturing all actual positives.  
- **F1 Score** = 2 * (Precision * Recall) / (Precision + Recall) â€” balances precision and recall.  
- **ROC-AUC Score** â€” measures how well the model separates classes.

---

### 12. How does class imbalance affect Logistic Regression?

- Logistic Regression tends to **favor the majority class**.  
- It may show **high accuracy** but **poor recall** for the minority class.  
- **Solutions:**  
  - **class_weight='balanced'** in `sklearn`.  
  - **Resampling**: Oversample minority or undersample majority.  
  - Use **metrics like F1 score or AUC-ROC** instead of accuracy.

---

### 13. What is Hyperparameter Tuning in Logistic Regression?

- Adjusts parameters to improve model performance.  
- Key hyperparameters:  
  - **C** (inverse of regularization strength): Smaller values = stronger regularization.  
  - **solver** (optimization algorithm): e.g., 'liblinear', 'saga'.  
  - **max_iter**: Max iterations for convergence.  
- **Methods:**  
  - **Grid Search**: Tries all possible combinations.  
  - **Randomized Search**: Randomly picks combinations.

---

### 14. What are different solvers in Logistic Regression? Which one should be used?

- **liblinear** â€” Good for small datasets and L1 regularization.  
- **saga** â€” For large datasets, supports both L1 and L2 regularization.  
- **lbfgs** â€” Used for multiclass classification (Softmax).  
- **newton-cg** â€” Suitable for L2 regularization.  
- **sag** â€” Efficient for large datasets.  

Choose based on dataset size and type of regularization.

---

### 15. How is Logistic Regression extended for multiclass classification?

- **One-vs-Rest (OvR):**  
  - Trains **n** binary classifiers (one for each class vs. the rest).  
- **Softmax Regression (Multinomial Logistic Regression):**  
  - Calculates probabilities for all classes at once.  
- In `sklearn`:  
  - OvR: `multi_class='ovr'`  
  - Softmax: `multi_class='multinomial', solver='lbfgs'`

---

### 16. What are the advantages and disadvantages of Logistic Regression?

**Advantages:**  
- Easy to implement and interpret.  
- Fast training and prediction.  
- Works well for linearly separable data.  

**Disadvantages:**  
- Assumes a linear relationship between features and log-odds.  
- Canâ€™t handle complex relationships without feature engineering.  
- Sensitive to outliers.

---

### 17. What are some use cases of Logistic Regression?

- **Medical diagnosis** â€” Predicting diseases like diabetes.  
- **Finance** â€” Loan default prediction.  
- **Marketing** â€” Customer purchase likelihood.  
- **Spam filters** â€” Classifying emails as spam or not.

---

### 18. What is the difference between Softmax Regression and Logistic Regression?

- **Logistic Regression**:  
  - Binary classification (class 0 or 1).  
  - Outputs a single probability \( P(y=1|x) \).  

- **Softmax Regression**:  
  - Multiclass classification.  
  - Outputs a probability distribution across all classes, summing to 1.

---

### 19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?

- **OvR** â€” Use when:  
  - Classes **arenâ€™t mutually exclusive**.  
  - You want simpler, independent classifiers.  

- **Softmax** â€” Use when:  
  - Classes are **mutually exclusive**.  
  - You want to predict **one class per instance**.  

---

### 20. How do we interpret coefficients in Logistic Regression?

- Each coefficient \( beta_j \):  
  - Represents the **log-odds** change per unit increase in \( x_j \).  
- **Odds ratio**:  
  \[
  e^{beta_j}
  \]
  - If \( beta_j > 0 \): Increasing \( x_j \) increases class 1 probability.  
  - If \( beta_j < 0 \): Increasing \( x_j \) decreases class 1 probability.  

---

# **Practical**

**1.Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic
Regression, and prints the model accuracyC**
"""

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, RandomizedSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, cohen_kappa_score, precision_score, recall_score, f1_score
import pandas as pd
import numpy as np

# Load the Iris dataset
data = load_iris()
X = data.data
y = data.target

# Split the dataset into training (70%) and testing (30%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 1. Logistic Regression Model
model = LogisticRegression(max_iter=200)
model.fit(X_train, y_train)

# Predictions and Accuracy
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Q1 \n Model Accuracy: {accuracy:.2f}")

# 2. L1 Regularization (Lasso)
l1_model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)
l1_model.fit(X_train, y_train)
l1_pred = l1_model.predict(X_test)
l1_accuracy = accuracy_score(y_test, l1_pred)
print(f"Q2 \n L1 Regularization Model Accuracy: {l1_accuracy:.2f}")

# 3. L2 Regularization (Ridge)
l2_model = LogisticRegression(penalty='l2', max_iter=200)
l2_model.fit(X_train, y_train)
l2_pred = l2_model.predict(X_test)
l2_accuracy = accuracy_score(y_test, l2_pred)
print(f"Q3 \n L2 Regularization Model Accuracy: {l2_accuracy:.2f}")
print(f"L2 Coefficients: {l2_model.coef_}")

# 4. Elastic Net Regularization
elastic_model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200)
elastic_model.fit(X_train, y_train)
elastic_pred = elastic_model.predict(X_test)
elastic_accuracy = accuracy_score(y_test, elastic_pred)
print(f"Q4 \n Elastic Net Model Accuracy: {elastic_accuracy:.2f}")

# 5. Multiclass Classification using OvR
ovr_model = LogisticRegression(multi_class='ovr', max_iter=200)
ovr_model.fit(X_train, y_train)
ovr_pred = ovr_model.predict(X_test)
ovr_accuracy = accuracy_score(y_test, ovr_pred)
print(f"Q5 \n OvR Model Accuracy: {ovr_accuracy:.2f}")

# 6. Hyperparameter Tuning using GridSearchCV
param_grid = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}
grid_search = GridSearchCV(LogisticRegression(max_iter=200), param_grid, cv=5)
grid_search.fit(X_train, y_train)
print(f"Q6 \n Best Parameters: {grid_search.best_params_}")
print(f"Best Accuracy: {grid_search.best_score_:.2f}")

# 7. Stratified K-Fold Cross-Validation
skf = StratifiedKFold(n_splits=5)
kf_accuracies = []
for train_index, test_index in skf.split(X, y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    model.fit(X_train, y_train)
    kf_accuracies.append(model.score(X_test, y_test))
print(f"Q7 \n Average Accuracy from Stratified K-Fold: {sum(kf_accuracies)/len(kf_accuracies):.2f}")

#8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.
df = pd.read_csv('IRIS.csv')
X = df.drop('species', axis=1).values
y = pd.factorize(df['species'])[0]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
model.fit(X_train, y_train)
csv_pred = model.predict(X_test)
csv_accuracy = accuracy_score(y_test, csv_pred)
print(f"Q8 \n CSV Dataset Model Accuracy: {csv_accuracy:.2f}")

#9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracyM
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Sample data â€” replace with your actual dataset
X, y = np.random.rand(200, 5), np.random.randint(0, 2, 200)

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the logistic regression model
log_reg = LogisticRegression(max_iter=1000)

# Define the hyperparameter grid
param_grid = {
    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga'],
    'penalty': ['l2', 'l1', 'elasticnet'],
    'C': np.logspace(-4, 4, 10)
}

# Remove invalid combinations
valid_params = [
    {'solver': 'newton-cg', 'penalty': 'l2', 'C': C} for C in np.logspace(-4, 4, 10)
] + [
    {'solver': 'lbfgs', 'penalty': 'l2', 'C': C} for C in np.logspace(-4, 4, 10)
] + [
    {'solver': 'liblinear', 'penalty': penalty, 'C': C} for penalty in ['l1', 'l2'] for C in np.logspace(-4, 4, 10)
] + [
    {'solver': 'saga', 'penalty': penalty, 'C': C} for penalty in ['l1', 'l2', 'elasticnet'] for C in np.logspace(-4, 4, 10)
]

# Initialize GridSearchCV
grid_search = GridSearchCV(log_reg, valid_params, cv=5, scoring='accuracy', n_jobs=-1)

# Fit GridSearchCV
grid_search.fit(X_train, y_train)

# Best hyperparameters and cross-validation accuracy
print("Best Hyperparameters:", grid_search.best_params_)
print("Best Cross-validation Accuracy:", grid_search.best_score_)

# Evaluate on test set
y_pred = grid_search.best_estimator_.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred)
print("Test Set Accuracy:", test_accuracy)

#10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.

# Import necessary libraries
import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsOneClassifier
from sklearn.metrics import accuracy_score

# Load the Iris dataset
data = load_iris()
X = data.data
y = data.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create One-vs-One Logistic Regression model
ovo_clf = OneVsOneClassifier(LogisticRegression(max_iter=200))

# Train the model
ovo_clf.fit(X_train, y_train)

# Predict the labels for test data
y_pred = ovo_clf.predict(X_test)

# Calculate and print the accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

#11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.
# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Load the Breast Cancer dataset
data = load_breast_cancer()
X = data.data
y = data.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create and train Logistic Regression model
log_reg = LogisticRegression(max_iter=200)
log_reg.fit(X_train, y_train)

# Predict the labels for test data
y_pred = log_reg.predict(X_test)

# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Visualize confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)
disp.plot(cmap='Blues')
plt.title('Confusion Matrix')
plt.show()

#12.Question: Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.

# Import necessary libraries
import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_score, recall_score, f1_score

# Load the Breast Cancer dataset
data = load_breast_cancer()
X = data.data
y = data.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create and train Logistic Regression model
log_reg = LogisticRegression(max_iter=200)
log_reg.fit(X_train, y_train)

# Predict the labels for test data
y_pred = log_reg.predict(X_test)

# Calculate Precision, Recall, and F1-Score
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Display the results
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1-Score: {f1:.2f}")

#13. Question: Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.

# Import necessary libraries
import numpy as np
import pandas as pd
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_score, recall_score, f1_score

# Create imbalanced dataset
X, y = make_classification(n_classes=2, class_sep=2, weights=[0.9, 0.1],
                           n_informative=3, n_redundant=1, flip_y=0,
                           n_features=5, n_clusters_per_class=1, n_samples=1000, random_state=42)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create and train Logistic Regression model with class weights
log_reg = LogisticRegression(class_weight='balanced', max_iter=200)
log_reg.fit(X_train, y_train)

# Predict the labels for test data
y_pred = log_reg.predict(X_test)

# Calculate Precision, Recall, and F1-Score
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Display the results
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1-Score: {f1:.2f}")

#Question 14: Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.

# Import libraries
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score

# Load Titanic dataset
data = sns.load_dataset('titanic')

# Select relevant features
features = ['sex', 'age', 'fare', 'embarked']
data = data[features + ['survived']]

# Handle missing values
imputer = SimpleImputer(strategy='mean')
data['age'] = imputer.fit_transform(data[['age']])
data['embarked'].fillna(data['embarked'].mode()[0], inplace=True)

# Encode categorical features
le = LabelEncoder()
data['sex'] = le.fit_transform(data['sex'])
data['embarked'] = le.fit_transform(data['embarked'])

# Split into features and target
X = data[features]
y = data['survived']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train Logistic Regression model
model = LogisticRegression(max_iter=200)
model.fit(X_train, y_train)

# Evaluate performance
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))

#15. Question : Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.

# Import libraries
from sklearn.preprocessing import StandardScaler

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train Logistic Regression with scaling
model_scaled = LogisticRegression(max_iter=200)
model_scaled.fit(X_train_scaled, y_train)

# Evaluate performance
y_pred_scaled = model_scaled.predict(X_test_scaled)
print("Accuracy with scaling:", accuracy_score(y_test, y_pred_scaled))

# Compare with non-scaled model
model_no_scaling = LogisticRegression(max_iter=200)
model_no_scaling.fit(X_train, y_train)
y_pred_no_scaling = model_no_scaling.predict(X_test)
print("Accuracy without scaling:", accuracy_score(y_test, y_pred_no_scaling))

